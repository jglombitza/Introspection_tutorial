<img class="center" width="300" src="https://www.johndcook.com/blackbox.jpeg">
<sub><sup>Image credit: John D. Cook</sup></sub>

# Interpretability and Deep Learning
## - ErUM train the trainers workshop 2022 -

The slides of the lecture can be found here: https://indico.scc.kit.edu/event/2645/contributions/9862/




### Solution I: Activation Maximization
<a target="_blank" rel="noopener noreferrer" href="https://colab.research.google.com/github/jglombitza/Introspection_tutorial/blob/solutions/activation_maximization_solutions.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="drawing" width="180"/> </a>


### Solution II: Prediction Analysis
<a target="_blank" rel="noopener noreferrer" href="https://colab.research.google.com/github/jglombitza/Introspection_tutorial/blob/solutions/discriminative_localization_solution.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="drawing" width="180"/> </a>

#### Disclaimer
The examples are inspired from Deep Learning for Physics Research.
Find the full exercise page at: http://deeplearningphysics.org  




##### Solutions for the tasks can be found in the respective "solution" branch


## Software requirements:
Tested for tf.keras 2.8.0

